{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11de6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#RL Fan Controller\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RLFanController:\n",
    "    #Q-Learning controller\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.2\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.actions = [-100, 0, 100]\n",
    "        self.q_table = {}\n",
    "\n",
    "        self.min_fan_speed = 1000\n",
    "        self.max_fan_speed = 2000\n",
    "\n",
    "    def _discretize_state(self, co2, temp, fan_speed, gas_flow):\n",
    "        #State includes flow sensor reading\n",
    "        co2_bin = int(co2 / 50)\n",
    "        temp_bin = int(temp / 5)\n",
    "        fan_bin = int(fan_speed / 100)\n",
    "        flow_bin = int(gas_flow / 50)  # ← FLOW SENSOR IN STATE\n",
    "\n",
    "        return (co2_bin, temp_bin, fan_bin, flow_bin)\n",
    "\n",
    "    def get_action(self, state, explore=True):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "\n",
    "        if explore and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, len(self.actions))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def calculate_reward(self, weight_captured, fan_speed, gas_flow):\n",
    "        # Reward considers flow rate\n",
    "        # Maximize capture\n",
    "        capture_reward = weight_captured * 20\n",
    "\n",
    "        # Minimize energy\n",
    "        energy_penalty = -(fan_speed / self.max_fan_speed) * 3\n",
    "\n",
    "        # Bonus for optimal flow rate (400-600 L/min)\n",
    "        if 400 <= gas_flow <= 600:\n",
    "            flow_bonus = 2\n",
    "        else:\n",
    "            flow_bonus = -1\n",
    "\n",
    "        return capture_reward + energy_penalty + flow_bonus\n",
    "\n",
    "    def update_q_value(self, state, action_idx, reward, next_state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.actions))\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(len(self.actions))\n",
    "\n",
    "        current_q = self.q_table[state][action_idx]\n",
    "        max_next_q = np.max(self.q_table[next_state])\n",
    "\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action_idx] = new_q\n",
    "\n",
    "    def control_fan_speed(self, co2, temp, current_fan_speed, gas_flow, explore=False):\n",
    "        #Control uses flow sensor data\n",
    "        state = self._discretize_state(co2, temp, current_fan_speed, gas_flow)\n",
    "        action_idx = self.get_action(state, explore=explore)\n",
    "        fan_speed_change = self.actions[action_idx]\n",
    "\n",
    "        new_fan_speed = current_fan_speed + fan_speed_change\n",
    "        new_fan_speed = max(self.min_fan_speed, min(self.max_fan_speed, new_fan_speed))\n",
    "\n",
    "        return new_fan_speed, action_idx, state\n",
    "\n",
    "    def train_episode(self, co2_data, temp_data, flow_data, weight_data):\n",
    "        #Training uses flow sensor data\n",
    "        total_reward = 0\n",
    "        current_fan_speed = 1500\n",
    "\n",
    "        for i in range(len(co2_data) - 1):\n",
    "            new_fan_speed, action_idx, state = self.control_fan_speed(\n",
    "                co2_data[i], temp_data[i], current_fan_speed,\n",
    "                flow_data[i], explore=True  # ← FLOW DATA\n",
    "            )\n",
    "\n",
    "            reward = self.calculate_reward(\n",
    "                weight_data[i+1], new_fan_speed, flow_data[i+1]  # ← FLOW DATA\n",
    "            )\n",
    "\n",
    "            next_state = self._discretize_state(\n",
    "                co2_data[i+1], temp_data[i+1],\n",
    "                new_fan_speed, flow_data[i+1]  # ← FLOW DATA\n",
    "            )\n",
    "\n",
    "            self.update_q_value(state, action_idx, reward, next_state)\n",
    "\n",
    "            current_fan_speed = new_fan_speed\n",
    "            total_reward += reward\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "print(\"RL Controller loaded (USES FLOW SENSOR)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
